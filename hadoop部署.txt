
一:Hadoop分布式搭建:
	hadoop官方文档:		http://hadoop.apache.org/docs/r2.7.7/

	1.环境准备:

		[root@nn01 ~]# systemctl stop firewalld.service 		//关闭防火墙
		[root@nn01 ~]# yum remove firewalld -y 		//卸载防火墙
		[root@nn01 ~]# setenforce 0					//关闭SELinux
		[root@nn01 ~]# yum  install java-1.8.0-openjdk-devel  -y		//安装环境
		[root@nn01 ~]# tar -xf hadoop-2.7.7.tar.gz			//解压hadoop
		[root@nn01 ~]# mv hadoop-2.7.7 /usr/local/hadoop		//移动
		[root@nn01 ~]# jps				
		22674 Jps

		[root@nn01 ~]# rpm -ql java-1.8.0-openjdk		//查看javaopenjdk安装目录
			/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64/jre/

		[root@nn01 ~]# vim /usr/local/hadoop/etc/hadoop/hadoop-env.sh			//编写hadoop配置文件
			25 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-2.b14.el7.x86_64/jre/		//java安装路径
			33 export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/usr/local/hadoop/etc/hadoop"}		//hadoop配置文件目录

			Hadoop修改SSH端口号
				hadoop-env.sh
				export HADOOP_SSH_OPTS="-p 1116"

	
	2.编辑hosts文件:
		[root@node1 ~]# yum  install java-1.8.0-openjdk-devel		//新的机器装环境及hosts文件
		[root@nn01 hadoop]# vim /etc/hosts		//要传给其他节点
			192.168.1.60 nn01
			192.168.1.61 node1
			192.168.1.62 node2
			192.168.1.63 node3	

		[root@node1 ~]# vim  /etc/ssh/ssh_config
			StrictHostKeyChecking no		//不要yes(添加)
	3.配置免密登录
		[root@nn01 .ssh]# ssh-keygen 
		[root@nn01 .ssh]# ssh-copy-id (node1,node2,node3 nn01)
		[root@nn01 .ssh]# ssh node1
		
	4.安装hadoop完全分布式	
		HDFS完全分布式系统配置
			环境变量配置文件:		hadoop-env.sh 	//export HADOOP_SSH_OPTS="-p 16022"  ##修改ssh端口号
			核心配置文件:			core-site.xml
			HDFS配置文件:		hdfs-site.xml
			节点配置文件:			slaves		
		
			
		4.1固定格式:
			<property>
		      <name></name>               
		      <value></value>
		 	</property>

		4.2 修改核心配置文件:
			官方文档位置:	http://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-common/core-default.xml
			[root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/core-site.xml		//修改核心配置文件
				
				
		4.3 修改HDFS配置文件:	
			官方文档位置:	http://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
			[root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml		//修改HDFS配置文件
			
		4.4 修改节点配置文件:
			[root@nn01 hadoop]# vim /usr/local/hadoop/etc/hadoop/slave		//节点配置文件,添加DataNode节点主机名称(只写主机名)
			

		4.5 同步配置 
			[root@nn01 hadoop]# for i in node{01..03}
			> do
			> rsync -aXSH -e 'ssh -p 1116' --delete /usr/local/hadoop ${i}:/usr/local/ &			//rsync同步
			> done
		4.6 启动集群
			[root@nn01 ~]# /usr/local/hadoop/bin/hadoop namenode -format		//格式化namenode
			[root@nn01 ~]# cd /usr/local/hadoop/sbin/
			[root@nn01 sbin]# ./start-dfs.sh 		//启动
			[root@nn01 sbin]# ./stop-dfs.sh			//停止

		4.7 查看状态
		[root@node1 ~]# jps
			23249 Jps
			23074 DataNode		//成功,没有就要排错
		[root@node1 hadoop]# ./bin/hdfs dfsadmin -report		//查看DataNode状态(能看到所有node)


	

	5.编辑分布式计算框架文件
		[root@nn01 hadoop]# cd  /usr/local/hadoop/etc/hadoop/
		[root@nn01 hadoop]# mv mapred-site.xml.template  mapred-site.xml
		[root@nn01 hadoop]# vim  mapred-site.xml		//编辑分布式计算框架
			<configuration>
			  <property>
				<name>mapreduce.framework.name</name>              
				<value>yarn</value>		//只支持单机local和集群yarn
			  </property>
			</configuration>
			
	6.编辑资源管理文件	
		[root@nn01 hadoop]# vim yarn-site.xml 		//编辑资源管理文件
			<configuration>
			  <property>
				<name>yarn.nodemanager.aux-services</name>		//使用哪个计算框架
				<value>mapreduce_shuffle</value>					//计算框架名称(要和开发人员沟通)
			  </property>
			<!-- Site specific YARN configuration properties -->
			  <property>
				<name>yarn.resourcemanager.hostname</name>    //resourcemanager地址
				<value>nn01</value>
			  </property>
			</configuration>

	7.同步配置文件
		[root@nn01 hadoop]# for i in node{1..3}; do rsync -aXSH --delete /usr/local/hadoop ${i}:/usr/local/ & done
		
	8.启动集群,验证状态
		[root@nn01 hadoop]# /usr/local/hadoop/sbin/start-yarn.sh		//启动集群
		
		[root@node1 hadoop]# /usr/local/hadoop/bin/yarn node -list		//查看是否组成集群
		19/08/16 09:50:56 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.1.60:8032
		Total Nodes:3
				 Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
			 node2:35134	        RUNNING	       node2:8042	                           0
			 node1:42069	        RUNNING	       node1:8042	                           0
			 node3:32969	        RUNNING	       node3:8042	                           0

	9. web访问hadoop
		http://ip:50070/			//--namenode web页面（nn01）
		http://ip:50090/		//--secondory namenode web页面（nn01）
		http://ip:50075/		//--datanode web页面（node1,node2,node3）
		http://ip:8088/		//--resourcemanager web页面（nn01）
		http://ip:8042/		//--nodemanager web页面（node1,node2,node3）



	*****	文件夹  进程一*****




		

二:集群使用
	哪台都可以,只要有hadoop就行
	[root@node1 hadoop]# ./bin/hadoop fs -mkdir /abc			/集群中创建文件夹
	[root@node1 hadoop]# ./bin/hadoop fs -ls /					//查看
	Found 1 items
	drwxr-xr-x   - root supergroup          0 2019-08-16 10:32 /abc
	[root@node1 hadoop]# ./bin/hadoop fs -touchz /ooxx		//创建文件(多z)
	[root@node1 hadoop]#  ./bin/hadoop fs -get /ooxx ./		//下载文件
	[root@node1 hadoop]# ./bin/hadoop fs -put ./*.txt /abc		//上传文件
	[root@node1 hadoop]#  ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar  \
								> wordcount /abc /bcd			//分析/abc文件夹将结果放到/bcd文件夹里
	[root@node1 hadoop]# ./bin/hadoop fs -ls /bcd		//查看/bcd文件夹
	[root@node1 hadoop]# ./bin/hadoop fs -cat /bcd/part-r-00000			//查看结果
	[root@node1 hadoop]# ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar \
								>  wordcount file:///etc/passwd /111		//分析本地文件passwd将结果放到HDFS的/111文件夹下

	fs支持的命令:
		[-appendToFile <localsrc> ... <dst>]
		[-cat [-ignoreCrc] <src> ...]
		[-checksum <src> ...]
		[-chgrp [-R] GROUP PATH...]
		[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
		[-chown [-R] [OWNER][:[GROUP]] PATH...]
		[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]
		[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
		[-count [-q] [-h] <path> ...]
		[-cp [-f] [-p | -p[topax]] <src> ... <dst>]
		[-createSnapshot <snapshotDir> [<snapshotName>]]
		[-deleteSnapshot <snapshotDir> <snapshotName>]
		[-df [-h] [<path> ...]]
		[-du [-s] [-h] <path> ...]
		[-expunge]
		[-find <path> ... <expression> ...]
		[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
		[-getfacl [-R] <path>]
		[-getfattr [-R] {-n name | -d} [-e en] <path>]
		[-getmerge [-nl] <src> <localdst>]
		[-help [cmd ...]]
		[-ls [-d] [-h] [-R] [<path> ...]]
		[-mkdir [-p] <path> ...]
		[-moveFromLocal <localsrc> ... <dst>]
		[-moveToLocal <src> <localdst>]
		[-mv <src> ... <dst>]
		[-put [-f] [-p] [-l] <localsrc> ... <dst>]
		[-renameSnapshot <snapshotDir> <oldName> <newName>]
		[-rm [-f] [-r|-R] [-skipTrash] <src> ...]
		[-rmdir [--ignore-fail-on-non-empty] <dir> ...]
		[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
		[-setfattr {-n name [-v value] | -x name} <path>]
		[-setrep [-R] [-w] <rep> <path> ...]
		[-stat [format] <path> ...]
		[-tail [-f] <file>]
		[-test -[defsz] <path>]
		[-text [-ignoreCrc] <src> ...]
		[-touchz <path> ...]
		[-truncate [-w] <length> <path> ...]
		[-usage [cmd ...]]

三 : HDFS节点管理
	3.1 增加新节点
		1 添加新机器,配置环境
			[root@node4 ~]# yum -y install java-1.8.0-openjdk-devel
			[root@node4 ~]# systemctl stop firewalld.service 		//关闭防火墙
			[root@node4 ~]# yum remove firewalld -y 		//卸载防火墙
			[root@node4 ~]# setenforce 0					//关闭SELinux
		
		2 编辑hosts文件
			[root@nn01 ~]# vim /etc/hosts
			192.168.1.64  node4
		
		3 设置免密登录
			[root@node4 ~]# ssh-copy																					
		
		4 修改NameNode的slave文件增加node节点
			[root@nn01 hadoop]# vim etc/hadoop/slaves
				node4
		
		5 拷贝文件
			[root@nn01 hadoop]# scp /etc/hosts node03:/etc/		//主机文件
			[root@nn01 hadoop]# scp -r /usr/local/hadoop/ node03:/usr/local/hadoop/

		6 启动新节点,查看状态
			[root@nn01 sbin]# ./hadoop-daemon.sh start datanode			//在新添加节点运行,启动DataNode命令
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report			//查看集群状态
				Name: 192.168.1.61:50010 (node1)	
				Name: 192.168.1.62:50010 (node2)
				Name: 192.168.1.63:50010 (node3)
				Name: 192.168.1.64:50010 (node4)
			
		7 设置同步带宽,同步数据
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin		//直接回车看可使用命令
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin -setBalancerBandwidth 50000000		//设置同步带宽,单位bytes
			[root@nn01 hadoop]# ./sbin/start-balancer.sh			//同步数据

	3.2 删除节点
		1 编辑配置文件
			[root@nn01 hadoop]# vim etc/hadoop/hdfs-site.xml		//编辑NameNode的hdfs-site.xml文件
				<property>
				  <name>dfs.hosts.exclude</name>
				  <value>/usr/local/hadoop/etc/hadoop/exclude</value>		//指定要删除的节点名称文件
				</property>
			[root@nn01 hadoop]# vim etc/hadoop/exclude				//添加要删除的节点名
			[root@nn01 hadoop]# vim etc/hadoop/slaves		//删除node4
			
		2 刷新数据
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin		//查看命令
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin -refreshNodes		//更新数据刷新节点
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report			//查看节点状态
				Decommission Status : Decommission in progress		//数据正在迁移
				Decommission Status : Decommissioned					//数据迁移成功(出现这个才能对要删除的节点下线)

四 : YARN节点管理
	[root@node4 hadoop]# ./sbin/yarn-daemon.sh start nodemanager			//启动节点(添加节点)
	[root@node4 hadoop]# ./sbin/yarn-daemon.sh stop nodemanager			//删除节点
		stopping nodemanager
		
	[root@node4 hadoop]# ./bin/yarn node -list				// 查看节点状态，还是有node4节点，要过一段时间才会消失
	19/08/16 14:46:51 INFO client.RMProxy: Connecting to ResourceManager at nn01/192.168.1.60:8032
	Total Nodes:5
		     Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers
		 node2:35134	        RUNNING	       node2:8042	                           0
		 node1:42069	        RUNNING	       node1:8042	                           0
		 node3:32969	        RUNNING	       node3:8042	                           0
		 node4:40707	        RUNNING	       node4:8042	                           0



	*****	文件夹  进程二*****


	
	五 : NFS网关
		5.1 添加用户
			[root@nn01 hadoop]# groupadd -g 800 nfsuser
			[root@nn01 hadoop]# useradd -u 800 -g 800 -r -d /var/hadoop nfsuser
			[root@nn01 hadoop]# id nfsuser
			uid=800(nfsuser) gid=800(nfsuser) 组=800(nfsuser)
			
			[root@nfsgw ~]# groupadd -g 800 nfsuser    (机器不是一台)
			[root@nfsgw ~]# mkdir /var/hadoop
			[root@nfsgw ~]# useradd -u 800 -g 800 -r -d /var/hadoop nfsuser
			[root@nfsgw ~]# id nfsuser
			uid=800(nfsuser) gid=800(nfsuser) 组=800(nfsuser)

		5.2 停止所有服务
			[root@nn01 hadoop]# ./sbin/stop-all.sh
			[root@nn01 hadoop]# jps		//查看
				8394 Jps

		5.3 修改配置
			[root@nn01 hadoop]# vim etc/hadoop/core-site.xml		//修改配置
				<configuration>
				  ......
				  <property>
					<name>hadoop.proxyuser.nfsuser.groups</name>		//挂载点用户所使用的组
					<value>*</value>
				  </property>
				  <property>
					<name>hadoop.proxyuser.nfsuser.hosts</name>			//挂载点主机地址
					<value>*</value>
				  </property>
				</configuration>

		5.4 同步配置到所有主机
			[root@nn01 hadoop]#  for i in node{1..3}; do rsync -aXSH --delete /usr/local/hadoop/etc \
									>  ${i}:/usr/local/hadoop & done		//只同步配置文件的命令

		5.5 启动hdfs
			[root@nn01 hadoop]# ./sbin/start-dfs.sh		//启动集群
			[root@nn01 hadoop]# jps 		//查看集群
				9383 NameNode
				9578 SecondaryNameNode
				9803 Jps
			[root@nn01 hadoop]# ./bin/hdfs dfsadmin -report		//查看器群节点
				Live datanodes (3):
				Name: 192.168.1.61:50010 (node1)
				Name: 192.168.1.62:50010 (node2)
				Name: 192.168.1.63:50010 (node3)

	六 : NFSGW配置
		6.1 环境配置
			卸载rpcbind  nfs-utils
				rpm -qa | grep rpcbind
				rpm -qa | grep nfs
			配置hosts文件
				[root@nfsgw ~]# vim /etc/hosts
					192.168.1.60 nn01
					192.168.1.61 node1
					192.168.1.62 node2
					192.168.1.63 node3
					192.168.1.65 nfsgw
			安装JAVA运行环境
				yum -y install java-1.8.0-openjdk-devel

			同步NameNode的hadoop文件夹到本机
				[root@nfsgw ~]# rsync -av 192.168.1.60:/usr/local/hadoop /usr/local/
				
		6.2 修改配置		
			配置文件hdfs-site.xml
				[root@nfsgw hadoop]# vim etc/hadoop/hdfs-site.xml
				<configuration>
				......
				<configuration>
				  <property>
					<name>nfs.exports.allowed.hosts</name> 		//共享文档权限(挂载用户)
					<value>* rw</value>
				  </property>
				  <property>
					<name>nfs.dump.dir</name>		//文件转储目录
					<value>/var/nfstmp</value>  	//指定文件路径
				  </property>
				</configuration>
				
			创建文件夹,修改所有权
				[root@nfsgw hadoop]# mkdir /var/nfstmp
				[root@nfsgw hadoop]# chown nfsuser:nfsuser /var/nfstmp
				[root@nfsgw hadoop]# ls -ld /var/nfstmp
				drwxr-xr-x 2 nfsuser nfsuser 6 8月  16 16:41 /var/nfstmp

		6.3 NFS启动与挂载
			NFS启动
				[root@nfsgw logs]# rm -rf /usr/local/hadoop/logs/*
				[root@nfsgw hadoop]# setfacl -m user:nfsuser:rwx logs/		//设置nfsuser可读权限
				[root@nfsgw hadoop]# getfacl logs/
					# file: logs/
					# owner: root
					# group: root
					user::rwx
					user:nfsuser:rwx
					group::r-x
					mask::rwx
						other::r-x
				[root@nfsgw hadoop]# ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap	//使用root启动portmap
				[root@nfsgw hadoop]# sudo -u nfsuser ./sbin/hadoop-daemon.sh --script \
										>  ./bin/hdfs start nfs3	 	//使用nfsuser用户启动nfs3
				[root@nfsgw hadoop]# jps		//查看状态
					23076 Portmap
					23162 Nfs3
					23213 Jps
				[root@nfsgw hadoop]# ls logs/		//查看启动日志(有hadoop-nfsuser-nfs3就成功)
					hadoop-nfsuser-nfs3-nfsgw.log  hadoop-root-portmap-nfsgw.log  SecurityAuth-nfsuser.audit
					hadoop-nfsuser-nfs3-nfsgw.out  hadoop-root-portmap-nfsgw.out  SecurityAuth-root.audit

			客户端挂载:
				[root@client ~]# yum -y install nfs-utils		//安装nfs软件
				[root@client ~]# mount -t nfs -o vers=3,proto=tcp,noacl,nolock,noatime,sync 192.168.1.65:/ /mnt/  (NFS地址)
													//	|版本三|协议tcp|禁止acl扩展权限|不支持NLM|禁用access time时间更新
				[root@client ~]# ls /mnt/
					111  abc  bcd  ooxx  system  tmp

			镜像启动nfs
				[root@nfsgw hadoop]# ./sbin/hadoop-daemon.sh --script ./bin/hdfs start portmap			//使用root启动portmap
				[root@nfsgw hadoop]# sudo -u nfsuser ./sbin/hadoop-daemon.sh --script   ./bin/hdfs start nfs3	 	//使用nfsuser用户启动nfs3

			hadoop集群调优:
				Hadoop修改SSH端口号
				hadoop-env.sh
				export HADOOP_SSH_OPTS="-p 1116"


	七:HDFS-HA集群配置
		7.1 环境准备
			修改IP
			修改主机名及主机名和IP地址的映射
			关闭防火墙
			ssh免密登录
			安装JDK，配置环境变量等

		7.2 规划集群
			nn01  			node01  			node02				node03				node04

			NameNode		NameNode	
			JournalNode		JournalNode			JournalNode			JournalNode			JournalNode
			DataNode		DataNode			DataNode 			DataNode 			DataNode
			ZK				ZK					ZK					ZK					ZK
							ResourceManager	
			NodeManager		NodeManager			NodeManager			NodeManager 		NodeManager

		7.3 配置Zookeeper集群
			1.	集群规划
				在nn01,node01,node02,node03,node04五个节点上部署Zookeeper。zookeeper安装台数为单数。

			2.	解压安装
				tzr xf zookeeper-3.4.14.tar.gz -C /usr/local/ 	//解压
				mkdir /data/zkData		//创建数据目录
				mv zoo_sample.cfg zoo.cfg 		//重命名conf这个目录下的zoo_sample.cfg为zoo.cfg

			3.	配置zoo.cfg文件
				（1）具体配置
						dataDir=/data/zkData	
					增加如下配置
						#######################cluster##########################
						server.2=nn01:2888:3888
						server.3=node01:2888:3888
						server.4=node02:2888:3888
						server.5=node03:2888:3888
						server.6=node04:2888:3888

				（2）配置参数解读
					Server.A=B:C:D。
					A是一个数字，表示这个是第几号服务器；
					B是这个服务器的IP地址；
					C是这个服务器与集群中的Leader服务器交换信息的端口；
					D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。
					集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。

			4.	集群操作
				（1）在/data/zkData目录下创建一个myid的文件,并写入文件中与server对应的编号。
					echo * > /data/zkData/myid

				（2）拷贝配置好的zookeeper到其他机器上
					scp -r -P 1116 zookeeper-3.4.14/ root@node01:/usr/local/
					并分别修改myid文件中内容为3、4

				（3）分别启动zookeeper
					/usr/local/zookeeper-3.4.14/bin/zkServer.sh start

				（4）查看状态
					Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg
					Mode: follower
					Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg
					Mode: leader  //领导者（主节点）


		7.4 配置HDFS-HA集群
			1. 配置core-site.xml
				<configuration>
					<!-- 把两个NameNode）的地址组装成一个集群mycluster -->
					<property>
					  <name>fs.defaultFS</name>
					  <value>hdfs://mycluster</value>
					</property>
				</configuration>

			2.  配置hdfs-site.xml
				<!-- 完全分布式集群名称 -->
				<property>
					<name>dfs.nameservices</name>
					<value>mycluster</value>
				</property>

				<!-- 集群中NameNode节点都有哪些 -->
				<property>
					<name>dfs.ha.namenodes.mycluster</name>
					<value>nn1,nn2</value>
				</property>

				<!-- nn1的RPC通信地址 -->
				<property>
					<name>dfs.namenode.rpc-address.mycluster.nn1</name>
					<value>nn01:9000</value>
				</property>

				<!-- nn2的RPC通信地址 -->
				<property>
					<name>dfs.namenode.rpc-address.mycluster.nn2</name>
					<value>node01:9000</value>
				</property>

				<!-- nn1的http通信地址 -->
				<property>
					<name>dfs.namenode.http-address.mycluster.nn1</name>
					<value>nn01:50070</value>
				</property>

				<!-- nn2的http通信地址 -->
				<property>
					<name>dfs.namenode.http-address.mycluster.nn2</name>
					<value>node01:50070</value>
				</property>

				<!-- 指定NameNode元数据在JournalNode上的存放位置 -->
				<property>
					<name>dfs.namenode.shared.edits.dir</name>
				<value>qjournal://nn01:8485;node01:8485;;node02:8485;node03:8485;node04:8485/mycluster</value>
				</property>

				<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
				<property>
					<name>dfs.ha.fencing.methods</name>
					<value>sshfence（[[username] [：port]]）</value>
				</property>

				<!-- 使用隔离机制时需要ssh无秘钥登录-->
				<property>
					<name>dfs.ha.fencing.ssh.private-key-files</name>
					<value>/home/atguigu/.ssh/id_rsa</value>
				</property>

				<!-- 声明journalnode服务器存储目录-->
				<property>
					<name>dfs.journalnode.edits.dir</name>
					<value>/data/jn</value>
				</property>

				<!-- 关闭权限检查-->
				<property>
					<name>dfs.permissions.enable</name>
					<value>false</value>
				</property>
				<!-- 访问代理类：client，mycluster，active配置失败自动切换实现方式-->
		        <property>
		                <name>dfs.client.failover.proxy.provider.mycluster</name>
		        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
		        </property>


			3. 拷贝配置好的hadoop环境到其他节点

		7.5 启动HDFS-HA集群
			1.	在各个JournalNode节点上，输入以下命令启动journalnode服务
				/usr/local/hadoop/sbin/hadoop-daemon.sh start journalnode

			2.	在[nn1]上，对其进行格式化，并启动
				/usr/local/hadoop/bin/hdfs namenode -format
				/usr/local/hadoop/sbin/hadoop-daemon.sh start namenode

			3.	在[nn2]上，同步nn1的元数据信息
				/usr/local/hadoop/bin/hdfs namenode -bootstrapStandby

			4.	启动[nn2]
				/usr/local/hadoop/sbin/hadoop-daemon.sh start namenode

			5. 查看web页面显示
				nn01:50070
				node01:50070

			6.	在[nn1]上，启动所有datanode
				sbin/hadoop-daemons.sh start datanode

			7.	将[nn1]切换为Active
				bin/hdfs haadmin -transitionToActive nn1

				查看是否Active
				bin/hdfs haadmin -getServiceState nn1

		7.6 配置HDFS-HA自动故障转移
			1.	具体配置
				（1）在hdfs-site.xml中增加
					<property>
						<name>dfs.ha.automatic-failover.enabled</name>
						<value>true</value>
					</property>

				（2）在core-site.xml文件中增加
					<property>
					    <name>ha.zookeeper.quorum</name>
					    <value>nn01:2181,node01:2181,node02:2181,node03:2181,node04:2181</value>
					</property>

				（3） 同步
				for i in 1 2 3 4 ; do scp -r  /usr/local/hadoop/etc/hadoop/core-site.xml /usr/local/hadoop/etc/hadoop/hdfs-site.xml node0$i:/usr/local/; done
			
					

			2. 启动
				（1）关闭所有HDFS服务：
					/usr/local/hadoop/sbin/stop-dfs.sh

				（2）启动Zookeeper集群：
					/usr/local/zookeeper-3.4.14/bin/zkServer.sh start

				（3）初始化HA在Zookeeper中状态：
					/usr/local/hadoop/bin/hdfs zkfc -formatZK

				（4）启动HDFS服务：
					/usr/local/hadoop/sbin/start-dfs.sh

				（5）在各个NameNode节点上启动DFSZK Failover Controller，先启动的NameNode就是Active NameNode
					sbin/hadoop-daemin.sh start zkfc

			3.	验证
				（1）将Active NameNode进程kill
					kill -9 namenode的进程id
					
				（2）将Active NameNode机器断开网络
					service network stop

			4. 故障排除
				1. 不能自动切换
					原因1: 没装包 自动切换依赖psmisc
					yum -y install psmisc

					原因2: ssh端口不是默认的
					更改端口或者指定端口
					<property>
						<name>dfs.ha.fencing.methods</name>
						<value>sshfence([[username] [：port]])</value>
					</property>
					

	八. YARN-HA配置
		
		8.1 具体配置
			（1）yarn-site.xml
				<configuration>
				    <property>
				        <name>yarn.nodemanager.aux-services</name>
				        <value>mapreduce_shuffle</value>
				    </property>

				    <!--启用resourcemanager ha-->
				    <property>
				        <name>yarn.resourcemanager.ha.enabled</name>
				        <value>true</value>
				    </property>
				 
				    <!--声明两台resourcemanager的地址-->
				    <property>
				        <name>yarn.resourcemanager.cluster-id</name>
				        <value>cluster-yarn1</value>
				    </property>

				    <property>
				        <name>yarn.resourcemanager.ha.rm-ids</name>
				        <value>rm1,rm2</value>
				    </property>

				    <property>
				        <name>yarn.resourcemanager.hostname.rm1</name>
				        <value>hadoop102</value>
				    </property>

				    <property>
				        <name>yarn.resourcemanager.hostname.rm2</name>
				        <value>hadoop103</value>
				    </property>
				 
				    <!--指定zookeeper集群的地址--> 
				    <property>
				        <name>yarn.resourcemanager.zk-address</name>
				        <value>hadoop102:2181,hadoop103:2181,hadoop104:2181</value>
				    </property>

				    <!--启用自动恢复--> 
				    <property>
				        <name>yarn.resourcemanager.recovery.enabled</name>
				        <value>true</value>
				    </property>
				 
				    <!--指定resourcemanager的状态信息存储在zookeeper集群--> 
				    <property>
				        <name>yarn.resourcemanager.store.class</name>     <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
					</property>
				</configuration>

		8.2 启动YARN 
			（1）在hadoop102中执行：
				sbin/start-yarn.sh

			（2）在hadoop103中执行：
				sbin/yarn-daemon.sh start resourcemanager

			（3）查看服务状态，如图3-24所示
				bin/yarn rmadmin -getServiceState rm1










